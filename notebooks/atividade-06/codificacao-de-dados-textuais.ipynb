{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65c5032-a31d-449e-a6ec-a53d5b848d20",
   "metadata": {},
   "source": [
    "## Codificação de dados textuais\n",
    "\n",
    "Os modelos estatísticos ou de deep learning utilizam calculos matemáticos para fazer as correlações e gerar as previsões, porém algumas variáveis não são numéricas ou não estão organizadas de uma maneira fixa. Então para que o nosso algorítimo possa processar aquele dado, ele precisa ser numérico. Porém os textos, são um tipo de dado não estruturado, sendo difícil para o computador conseguir entendê-los e analisá-los. Por isso uma grande parte do processamento de linguagem natural ocorre convertendo o texto em uma informação numérica. Esse processo de conversão pode ser feito de duas maneiras, utilizando o método saco de palavras (*bag-of-words*) ou o método TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565d341-bf7f-43ac-85b7-92365039c06e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Saco de palavras (*bag-of-words*)\n",
    "\n",
    "É um método de processamento de linguagem natural para modelagem de textos, onde não se leva em consideração a estrutura nem a ordem das palavras no texto, apenas a frequência com que elas aparecem e se elas aparecem. Um dos motivos para se utilizar o *bag-of-words* é porque os algoritimos para aprendizagem de máquina preferem entradas estruturadas, bem definidas e de comprimento fixo, algo que não ocorre em um texto. Então utilizando esse método podemos converter textos com comprimentos variáveis em textos com comprimentos fixos (seu vetor equivalente de números).\n",
    "\n",
    "Para gerar um modelo de *bag-of-words* precisamos realizar três passos:\n",
    "- Selecionar os dados\n",
    "- Gerar o vocabulário\n",
    "- Formar vetores a partir do documento\n",
    "\n",
    "#### Selecionando os dados: pré-processamento\n",
    "Na fase de pré-processamento devemos:\n",
    "- Converter todas as letras em minúsculas\n",
    "    Essa etapa é importante pois nossa maquina tende a interpretar palavras iguais como sendo diferentes caso tenham maiúsculas ou minúsculas, ex.: 'Manhã' e 'manhã'.\n",
    "- Remover os caracteres especiais e palavras irrelecantes do texto, ex.: 'é', 'a', 'o'...\n",
    "    Para a máquina, a pontuação de um texto não é relevante, então vamos guardar apenas as palavras.\n",
    "\n",
    "\n",
    "#### Gerando vocabulário\n",
    "Vocabulário, nada mais é que a coleção de todas as palavras que existem em um texto, então a ideia do código é:\n",
    "- Fazer uma lista de todas as palavras do nosso vocabulário modelo.\n",
    "- Fazer um loop para percorrer o texto inteiro\n",
    "- Criar uma condicional para verificar se a palavra está na lista- o vocabulário conta apenas com ocorrências únicas\n",
    "- adicionar a palavra, caso ela não esteja na lista\n",
    "\n",
    "#### Formando vetores\n",
    "Como o vocabulário tem um número fixo de palavras, podemos usar a representação de tamano fixo equivalente a esse número, ou seja, um vetor onde cada elemento corresponderá a uma palavra do vocabulário. Existem diversas maneiras de se fazer essa atribuição, mas a mais básica é atribuindo 1 caso a palavra apareça e 0 caso não apareça\n",
    "\n",
    "Assim, o passo a passo para codificar é:\n",
    "- Criar uma lista que representa o vetor\n",
    "- Fazer um loop para percorrer todas as palavras do vocabulário\n",
    "- Se a palavra estiver no documento, adicionar 1 à lista e 0 caso não tenha\n",
    "- Transformar a lista final em um array e retornar.\n",
    "\n",
    "### Limitações do *bag-of-words*\n",
    "Embora *bag-of-words* seja bastante eficiente e fácil de implementar, ainda existem algumas desvantagens nesta técnica:\n",
    "\n",
    "1. O modelo ignora as informações de localização da palavra. A informação de localização é uma informação muito importante no texto. Por exemplo, “está fechado hoje.” e “hoje está fecahdo?”, têm exatamente a mesma representação vetorial no modelo BoW.\n",
    "\n",
    "2. *bag-of-words* não respeita a semântica das palavras. Por exemplo, as palavras 'dinâmica' e 'atividade' são frequentemente usadas no mesmo contexto. No entanto, os vetores correspondentes a essas palavras são bastante diferentes no modelo do saco de palavras. O problema se torna mais sério ao modelar frases. Ex: “Comprar carros usados” e “adiquirir antiguidades automobilísticas” são representados por vetores totalmente diferentes no modelo *bag-of-words*.\n",
    "\n",
    "3. A variedade de vocabulário é um grande problema enfrentado pelo modelo *bag-of-words*. Por exemplo, se o modelo se deparar com uma nova palavra que ainda não viu, em vez disso, dizemos uma palavra rara, mas informativa, como biblioclepta (significa aquele que rouba livros). O modelo BoW provavelmente acabará ignorando essa palavra, pois essa palavra ainda não foi vista pelo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676cc94-b9ca-4d70-b0f5-10ff3841717b",
   "metadata": {},
   "source": [
    "### Term Frequency Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "São medidas estatísticas para medir o quão importante uma palavra é em um documento. Utilizando ele podemos perceber a importância de uma palavra por meio da pontuação, para isso multiplica-se duas métricas diferentes:\n",
    "\n",
    "- Term Frequency (a frequência do termo), que mede a frequência com que um termo ocorre num documento. Existem várias maneiras de calcular essa frequência, sendo a mais simples uma contagem bruta de ocorrências em que uma palavra aparece em um documento.\n",
    "\n",
    "- Inverse Document Frequency (inverso da frequência nos documentos), que mede o quão importante um termo é no contexto de todos os documentos. Isso sugere quão comum ou rara é uma palavra em todo o conjunto de documentos. Quanto mais próximo de 0, mais comum é a palavra. Essa métrica pode ser calculada tomando o número total de documentos, dividindo-o pelo número de documentos que contêm uma palavra e calculando o logaritmo.\n",
    "\n",
    "Ou seja, para o TF-IDF, quanto mais frequente uma palavra é em seu documento, mais importante ela tende a ser, porem isso vai depender da repetição dela ao longo de todos os documentos analisados.\n",
    "\n",
    "#### Implementando\n",
    "\n",
    "Para a implementação de TF-IDF precisamos seguir passos muito semelhantes ao que fizemos com *bag-of-words*:\n",
    "\n",
    "- Selecionar os dados e pré-processar o texto\n",
    "- Gerar um vocabulário\n",
    "- Gerar um dicionário de frequência desses termos\n",
    "\n",
    "As duas primeiras etapas se assemelham ao *bag-of-words*, então para criar o dicionário de frequência devemos fazer uma contagem da quantidade de ocorrências de cada palavra.\n",
    "\n",
    "Para obter a pontuação TF-IDF, basta então multiplicar essas duas medidas. Assim, quanto maior a TF e a IDF, obteremos uma pontuação maior, ou seja, as palavras consideradas mais relevantes serão aquelas que aparecem muito em um documento, mas pouco em outros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0da2bf-0088-4404-858a-a6c456514ffa",
   "metadata": {},
   "source": [
    "### Prática\n",
    "\n",
    "#### 1. *Bag-of-Words*\n",
    "\n",
    "-  Selecionando os dados e fazendo o pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7affc367-e66f-48e5-8415-a9c17e860762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /home/rafaelxavier/.cache/pypoetry/virtualenvs/src-bLkq73BS-py3.8/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/rafaelxavier/.cache/pypoetry/virtualenvs/src-bLkq73BS-py3.8/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.9.13-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.9.13 tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "# instalar nltk\n",
    "# !pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e25ef63d-5298-458d-8afb-cf230f73c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando módulos e bibliotecas\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "086d2315-4755-432a-9acb-2dccfcbcc890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/rafaelxavier/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "679849f8-661c-42b0-9bd7-8a20f597a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\n",
    "    \"John likes\",\n",
    "    \"likes to\",\n",
    "    \"to watch\",\n",
    "    \"watch movies\",\n",
    "    \"Mary likes\",\n",
    "    \"likes movies\",\n",
    "    \"movies too\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d87c85-f576-48bd-847b-4009f2cd846a",
   "metadata": {},
   "source": [
    "- Colocando todas as letras em minúsculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43f70a17-c9e7-4192-9461-bc55b5b8be1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john likes',\n",
       " 'likes to',\n",
       " 'to watch',\n",
       " 'watch movies',\n",
       " 'mary likes',\n",
       " 'likes movies',\n",
       " 'movies too']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases = list(map(lambda item: item.lower(),frases))\n",
    "frases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae986ce-258d-4d9d-a514-86e46568ac64",
   "metadata": {},
   "source": [
    "- Removendo acentos e palavras irrelevantes: stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "617e1248-4150-466c-8a13-3afc676e8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma lista com stopwords\n",
    "stopwords = ['to', 'too']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea08e9-c2b9-4b17-8a65-1f2be54f636f",
   "metadata": {},
   "source": [
    "- Formando vetores de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c5e40f82-ead3-48a9-bbc1-621d47e6fc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'likes',\n",
       " 'likes',\n",
       " 'to',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'watch',\n",
       " 'movies',\n",
       " 'mary',\n",
       " 'likes',\n",
       " 'likes',\n",
       " 'movies',\n",
       " 'movies',\n",
       " 'too']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = \" \".join(frases).split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "42f84863-add0-4f4a-8ea2-f861051fb163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'likes', 'to', 'watch', 'movies', 'mary', 'too'] \n",
      " 7\n"
     ]
    }
   ],
   "source": [
    "# Retirando palavras repetidas\n",
    "vocab = []\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab.append(token)\n",
    "\n",
    "print(vocab,\"\\n\",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2b6379b2-d664-4eed-a219-7b80bd4ba956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'likes', 'watch', 'movies', 'mary']\n"
     ]
    }
   ],
   "source": [
    "# filtrando stopwords\n",
    "filtered_vocab = []\n",
    "for item in vocab:\n",
    "    if item not in stopwords:\n",
    "        filtered_vocab.append(item)\n",
    "print(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "283f3f24-ef72-4ec8-97d4-be9652d06340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vetorizando(token,filtro):\n",
    "    vector = []\n",
    "    for item in filtro:\n",
    "        vector.append(token.count(item))\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "047fad6c-ada2-489d-b8d3-90852354c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vetorfinal = vetorizando(tokens,filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4bddd48e-7e0b-48a1-9531-59bc8893520c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 2, 3, 1]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetorfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "018bf6e1-ccbc-4f61-bc8b-6fbad7f386de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vetorizando_binario(token,filtro):\n",
    "    vector = []\n",
    "    for item in filtro:\n",
    "        if item in token:\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "            \n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "52c44107-a6b0-482e-b45f-298a4bb0732c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag-of-words: ['john', 'likes', 'watch', 'movies', 'mary']\n",
      "Vetor binário: [1, 1, 1, 1, 1]\n",
      "Vetor com a contagem de aparições: [1, 4, 2, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "vetorbinario = vetorizando_binario(tokens,filtered_vocab)\n",
    "print(f'bag-of-words: {filtered_vocab}')\n",
    "print(f'Vetor binário: {vetorbinario}')\n",
    "print(f'Vetor com a contagem de aparições: {vetorfinal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8e913-f36a-4a18-8b32-f9398427b951",
   "metadata": {},
   "source": [
    "### 2. TF-IDF\n",
    "\n",
    "- Selecionando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "80cbafdf-aaf1-4975-b87d-ace6978d1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando bibliotecas\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cb94c61b-d086-4cab-80ee-d58d413c27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\n",
    "    \"John likes\",\n",
    "    \"likes to\",\n",
    "    \"to watch\",\n",
    "    \"watch movies\",\n",
    "    \"Mary likes\",\n",
    "    \"likes movies\",\n",
    "    \"movies too\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0b5f9b40-0637-496c-a81d-642fdcc99f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo tf-idf, ngram_range=(2,2) porque a variável frases está dividida em bi-gramas\n",
    "tf_idf_vec_bi = TfidfVectorizer(use_idf=True,  \n",
    "                        smooth_idf=True,  \n",
    "                        ngram_range=(2,2),stop_words='english')\n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True,  \n",
    "                        smooth_idf=True,  \n",
    "                        ngram_range=(1,1),stop_words='english')\n",
    "\n",
    "# calculando tf-idf do texto\n",
    "tf_idf_data_bi = tf_idf_vec_bi.fit_transform(frases)\n",
    "tf_idf_data = tf_idf_vec.fit_transform(frases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3e930ae7-831f-47da-8d77-cb7a289d1c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 2)\t1.0\n",
      "  (5, 1)\t1.0\n",
      "['john likes' 'likes movies' 'mary likes' 'watch movies']\n",
      "\n",
      "\n",
      "  (0, 1)\t0.5244893845044374\n",
      "  (0, 0)\t0.8514169868766751\n",
      "  (1, 1)\t1.0\n",
      "  (2, 4)\t1.0\n",
      "  (3, 3)\t0.649749587643745\n",
      "  (3, 4)\t0.7601483232611799\n",
      "  (4, 2)\t0.8514169868766751\n",
      "  (4, 1)\t0.5244893845044374\n",
      "  (5, 3)\t0.7551128241024088\n",
      "  (5, 1)\t0.6555948618438714\n",
      "  (6, 3)\t1.0\n",
      "['john' 'likes' 'mary' 'movies' 'watch']\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_data_bi)\n",
    "print(tf_idf_vec_bi.get_feature_names_out())\n",
    "print('\\n')\n",
    "print(tf_idf_data)\n",
    "print(tf_idf_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c1364f81-8ed1-410d-82a0-976804fe6946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   john likes  likes movies  mary likes  watch movies\n",
      "0         1.0           0.0         0.0           0.0\n",
      "1         0.0           0.0         0.0           0.0\n",
      "2         0.0           0.0         0.0           0.0\n",
      "3         0.0           0.0         0.0           1.0\n",
      "4         0.0           0.0         1.0           0.0\n",
      "5         0.0           1.0         0.0           0.0\n",
      "6         0.0           0.0         0.0           0.0\n"
     ]
    }
   ],
   "source": [
    "# criando um dataframe com os resultados da analise tf-idf\n",
    "tf_idf_dataframe_bi=pd.DataFrame(tf_idf_data_bi.toarray(),columns=tf_idf_vec_bi.get_feature_names_out())\n",
    "print(tf_idf_dataframe_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6a52592f-8549-4c46-a4b3-4bf7930f86ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       john     likes      mary    movies     watch\n",
      "0  0.851417  0.524489  0.000000  0.000000  0.000000\n",
      "1  0.000000  1.000000  0.000000  0.000000  0.000000\n",
      "2  0.000000  0.000000  0.000000  0.000000  1.000000\n",
      "3  0.000000  0.000000  0.000000  0.649750  0.760148\n",
      "4  0.000000  0.524489  0.851417  0.000000  0.000000\n",
      "5  0.000000  0.655595  0.000000  0.755113  0.000000\n",
      "6  0.000000  0.000000  0.000000  1.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# criando um dataframe com os resultados da analise tf-idf\n",
    "tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names_out())\n",
    "print(tf_idf_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6cb20-03aa-444b-a126-d80da279b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
